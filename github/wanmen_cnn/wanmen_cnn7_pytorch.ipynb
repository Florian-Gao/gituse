{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## day1\n",
    "## pytorch 核心，设立计算图并自动计算\n",
    "### 梯度下降法是理解神经网络的核心\n",
    "### 具体步骤\n",
    "- 设定初始值\n",
    "- 求梯度\n",
    "- 在梯度方向进行更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单梯度下降法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=1\n",
    "learning_rate=0.1\n",
    "epochs=100\n",
    "#find minimum\n",
    "J_w=lambda w:w**2+2*w+1\n",
    "J_w(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9999999995925928"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    dw=2*w+2\n",
    "    w=w-learning_rate*dw\n",
    "    \n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad None data tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# pytorch 实现\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# 定义一个torch.float tensor 类似于array\n",
    "w=torch.Tensor([1])\n",
    "# 定义一个w，是一个变量，是建立计算图的起点\n",
    "w=Variable(w,requires_grad=True)\n",
    "#增加一个部分叫gradient，w.data和之前相同\n",
    "print('grad',w.grad,'data',w.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=w**2\n",
    "z=y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w->y->z的计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward做反向传导,实现z对w的导数\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 的本质，建立计算图，自动求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad tensor([22.])\n",
      "grad tensor([17.6000])\n",
      "grad tensor([14.0800])\n",
      "grad tensor([11.2640])\n",
      "grad tensor([9.0112])\n",
      "grad tensor([7.2090])\n",
      "grad tensor([5.7672])\n",
      "grad tensor([4.6137])\n",
      "grad tensor([3.6910])\n",
      "grad tensor([2.9528])\n",
      "grad tensor([2.3622])\n",
      "grad tensor([1.8898])\n",
      "grad tensor([1.5118])\n",
      "grad tensor([1.2095])\n",
      "grad tensor([0.9676])\n",
      "grad tensor([0.7741])\n",
      "grad tensor([0.6192])\n",
      "grad tensor([0.4954])\n",
      "grad tensor([0.3963])\n",
      "grad tensor([0.3171])\n",
      "grad tensor([0.2536])\n",
      "grad tensor([0.2029])\n",
      "grad tensor([0.1623])\n",
      "grad tensor([0.1299])\n",
      "grad tensor([0.1039])\n",
      "grad tensor([0.0831])\n",
      "grad tensor([0.0665])\n",
      "grad tensor([0.0532])\n",
      "grad tensor([0.0426])\n",
      "grad tensor([0.0340])\n",
      "grad tensor([0.0272])\n",
      "grad tensor([0.0218])\n",
      "grad tensor([0.0174])\n",
      "grad tensor([0.0139])\n",
      "grad tensor([0.0112])\n",
      "grad tensor([0.0089])\n",
      "grad tensor([0.0071])\n",
      "grad tensor([0.0057])\n",
      "grad tensor([0.0046])\n",
      "grad tensor([0.0037])\n",
      "grad tensor([0.0029])\n",
      "grad tensor([0.0023])\n",
      "grad tensor([0.0019])\n",
      "grad tensor([0.0015])\n",
      "grad tensor([0.0012])\n",
      "grad tensor([0.0010])\n",
      "grad tensor([0.0008])\n",
      "grad tensor([0.0006])\n",
      "grad tensor([0.0005])\n",
      "grad tensor([0.0004])\n",
      "grad tensor([0.0003])\n",
      "grad tensor([0.0003])\n",
      "grad tensor([0.0002])\n",
      "grad tensor([0.0002])\n",
      "grad tensor([0.0001])\n",
      "grad tensor([0.0001])\n",
      "grad tensor([8.2254e-05])\n",
      "grad tensor([6.5804e-05])\n",
      "grad tensor([5.2691e-05])\n",
      "grad tensor([4.2200e-05])\n",
      "grad tensor([3.3736e-05])\n",
      "grad tensor([2.6941e-05])\n",
      "grad tensor([2.1577e-05])\n",
      "grad tensor([1.7285e-05])\n",
      "grad tensor([1.3828e-05])\n",
      "grad tensor([1.1086e-05])\n",
      "grad tensor([8.8215e-06])\n",
      "grad tensor([7.0333e-06])\n",
      "grad tensor([5.6028e-06])\n",
      "grad tensor([4.5300e-06])\n",
      "grad tensor([3.5763e-06])\n",
      "grad tensor([2.8610e-06])\n",
      "grad tensor([2.2650e-06])\n",
      "grad tensor([1.7881e-06])\n",
      "grad tensor([1.4305e-06])\n",
      "grad tensor([1.1921e-06])\n",
      "grad tensor([9.5367e-07])\n",
      "grad tensor([7.1526e-07])\n",
      "grad tensor([5.9605e-07])\n",
      "grad tensor([4.7684e-07])\n",
      "grad tensor([3.5763e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n",
      "grad tensor([2.3842e-07])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=torch.Tensor([10])\n",
    "w=Variable(w,requires_grad=True) #这是最基础重要的\n",
    "learning_rate=0.1\n",
    "epochs=100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    J=w**2+2*w+1\n",
    "    #求导数\n",
    "    J.backward()\n",
    "    print('grad',w.grad.data)\n",
    "    #参数更新，放在梯度里\n",
    "    w.data=w.data-learning_rate*w.grad.data\n",
    "    #每一步导数都会积累在之前的梯度上\n",
    "    w.grad.data.zero_() # 梯度清零\n",
    "    \n",
    "w.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从一维到多维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用梯度下降法，优化$J(w,b)=w^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4.],\n",
       "         [6.]]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=Variable(torch.FloatTensor([[[2],[3]]]),requires_grad=True)\n",
    "# 必须改成如下\n",
    "#w=Variable(torch.FloatTensor([1]),requires_grad=True)\n",
    "#b=Variable(torch.FloatTensor([2]),requires_grad=True)\n",
    "J=torch.sum(w**2)\n",
    "print(J)\n",
    "J.backward()\n",
    "w.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w= 3e-45 b= 3e-45\n"
     ]
    }
   ],
   "source": [
    "#pytorch 梯度下降法最快速的实现\n",
    "w=Variable(torch.FloatTensor([2,5]),requires_grad=True)\n",
    "lr=0.1\n",
    "epochs=1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    J=torch.sum(w**2)\n",
    "    J.backward()\n",
    "    \n",
    "    #参数更新\n",
    "    assert(isinstance(w,Variable))\n",
    "    w.data=w.data-lr*w.grad.data\n",
    "    w.grad.data.zero_()\n",
    "    \n",
    "print('w=',w[0].data.numpy(),'b=',w[1].data.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## day2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性模型的三种代码对比\n",
    "- numpy\n",
    "- pytorch 求导\n",
    "- pytorch 神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.666666666666668\n",
      "0.08296296296296272\n",
      "0.00036872427983540356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9994074074074075"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "x_data=np.array([1,2,3])\n",
    "y_data=np.array([2,4,6])\n",
    "\n",
    "epochs=3\n",
    "lr=0.1\n",
    "w=0\n",
    "cost=[]\n",
    "for epoch in range(epochs):\n",
    "    #计算梯度\n",
    "    yhat=x_data*w\n",
    "    loss=np.average((yhat-y_data)**2)\n",
    "    print(loss)\n",
    "    cost.append(loss)\n",
    "    #优化公式, x.shape[0]显示行数\n",
    "    dw=-2*(y_data-yhat)@x_data.T/(x_data.shape[0])\n",
    "    # ？上面这行不是很明白\n",
    "    #参数更新\n",
    "    w=w-lr*dw\n",
    "    \n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch 基本方法\n",
    "torch.manual_seed(2)\n",
    "x_data=Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
    "y_data=Variable(torch.Tensor([[2.0],[4.0],[6.0]]))\n",
    "\n",
    "epochs=10\n",
    "lr=0.1\n",
    "w=Variable(torch.FloatTensor([0]),requires_grad=True)\n",
    "cost=[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #算梯度\n",
    "    yhat=x_data*w\n",
    "    loss=torch.mean((yhat-y_data)**2)\n",
    "    cost.append(loss.data.numpy())\n",
    "    loss.backward()\n",
    "    \n",
    "    #参数更新\n",
    "    w.data=w.data-lr*w.grad.data\n",
    "    w.grad.data.zero_()\n",
    "    \n",
    "w.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch 类方法，这一次，我们全面引入pytorch内的工具，一个是torch.nn.module写网络，MSEloss写损失，一个是torch.optim来进行梯度下降，使线性回归特别简单"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'Model' has no attribute 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-75eb2b37e9c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#定义loss和优化方法\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-75eb2b37e9c1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# one in and one out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# 前项传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'Model' has no attribute 'self'"
     ]
    }
   ],
   "source": [
    "# pytorch 求导来进行\n",
    "torch.manual_seed(2)\n",
    "x_data=Variable(torch.Tensor([[1.0],[2.0],[3.0]]))\n",
    "y_data=Variable(torch.Tensor([[2.0],[4.0],[6.0]]))\n",
    "\n",
    "# 定义模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model.self).__init__()\n",
    "        self.linear=torch.nn.Linear(1,1,bias=False)  # one in and one out\n",
    "    # 前项传播   \n",
    "    def forward(self,x):\n",
    "        y_pred=self.linear(x)\n",
    "        return y_pred\n",
    "model=Model()\n",
    "\n",
    "#定义loss和优化方法\n",
    "criterion=torch.nn.MSELoss(size_average=False)\n",
    "#等价公式\n",
    "#criterion=lambda yhat,y:torch.sum((yhat-y)**2)\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "\n",
    "epoch=20\n",
    "cost=[]\n",
    "for epoch in range(epochs):\n",
    "    #计算梯度\n",
    "    y_pred=model(x_data)\n",
    "    loss=criterion(y_pred,y_data)\n",
    "    cost.append(loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
