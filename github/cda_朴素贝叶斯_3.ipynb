{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 概述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.条件概率公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事件B发生的情况下，A发生(根据图来进行推断最为清晰)                   \n",
    "$p(A|B)=\\frac{p(A \\cap B)}{p(B)}=\\frac{p(B|A)p(A)}{p(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全概率公式(如果事件A1，A2.....An构成一个完备事件且都有正概率，那么对于任意一个事件B都有：)                      \n",
    "$p(B)=p(BA_1)+p(BA_2)+p(BA_3)+....p(BA_n)=p(B|A_1)p(A_1)+p(B|A_2)p(A_2)+p(B|A_3)p(A_3)+....p(B|A_n)p(A_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.贝叶斯推断"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(A|B)=p(A)\\frac{p(B|A)}{p(B)}$                \n",
    "$p(A_i|B)=p(A_i)\\frac{p(B|A_i)}{\\sum_{i=1}^n p(A_i)p(B|A_i)}$            \n",
    "<font color='red'>P(A)称为\"先验概率\"(Prior probability)，</font>即在B事件发生之前，我们对A事件概率的一个判断。        \n",
    "<font color='red'>P(A|B)称为\"后验概率\"(Posterior probability)，</font>即在B事件发生之后，我们对A事件概率的重新评估。                  \n",
    "<font color='red'>P(B|A)/P(B)称为\"可能性函数\"(Likely hood)，</font>这是一个调整因子，使得预估概率更接近真实概率。                 \n",
    "所以条件概率可以理解为:后验概率 = 先验概率 * 调整因子               \n",
    "如果\"可能性函数\">1，意味着\"先验概率\"被增强，事件A的发生的可能性变大;                \n",
    "如果\"可能性函数\"=1，意味着B事件无助于判断事件A的可能性;             \n",
    "如果\"可能性函数\"<1，意味着\"先验概率\"被削弱，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.嫁？还是不嫁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用上面的两个公式，比较简单可以分析出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. 朴素贝叶斯种类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在scikit-learn中，一共有3个朴素贝叶斯的分类算法。分别是GaussianNB，MultinomialNB和BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB就是先验为高斯分布(正态分布)的朴素贝叶斯，假设每个标签的数据都服从简单的正态分布<font color='red'>(高斯分布)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(X_j=x_j|Y=C_k)=\\frac{1}{\\sqrt{2\\pi\\sigma_k^2}}exp{(-\\frac{(x_j-\\mu_k)^2}{2\\sigma_k^2})}$       \n",
    "其中$C_k$为Y的第k类类别.$\\mu_k$和$\\sigma_k^2$为需要从训练集估计的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9736842105263158"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入包\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split \n",
    "#分类准确率分数是指所有分类正确的百分比\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#导入数据集\n",
    "from sklearn import datasets \n",
    "iris=datasets.load_iris()\n",
    "#print(iris)\n",
    "#切分数据集\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(iris.data,iris.target,random_state=12)\n",
    "\n",
    "#建模\n",
    "clf = GaussianNB() \n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "#在测试集上执行预测，proba导出的是每个样本属于某类的概率   \n",
    "#clf.predict(Xtest)返回某标签\n",
    "yP=clf.predict_proba(Xtest)\n",
    "#print(yP)\n",
    "#测试准确率\n",
    "accuracy_score(ytest, clf.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB就是先验为多项式分布的朴素贝叶斯。它假设特征是由一个简单多项式分布生成的。多项分布可以 描述各种类型样本出现次数的概率，因此<font color='red'>多项式朴素贝叶斯非常适合用于描述出现次数或者出现次数比例的特征。</font>  <font color='blue'> 该模型常用于文本分类，特征表示的是次数，例如某个词语的出现次数。</font>                \n",
    "多项式分布公式如下:                 \n",
    "$p(X_{j}=x_{jl}|Y=C_k)=\\frac{x_{jl}+\\lambda}{m_k+n\\lambda}$                \n",
    "其中，$p(X_{j}=x_{jl}|Y=C_k)$是第k个类别的第j维特征的第l个取值条件概率。$m_k$是训练集中输出为第k类的样本个数。$\\lambda$为一个大于0的常数，常常取为1，即拉普拉斯平滑。也可以取其他值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BernoulliNB就是先验为伯努利分布的朴素贝叶斯。假设特征的先验概率为二元伯努利分布，即如下式:         \n",
    "$p(X_{j}=x_{jl}|Y=C_k)=p(j|Y=C_k)x_{jl}+(1-p(j|Y=C_k)(1-x_{jl}))$             \n",
    "此时$l$只有两种取值。$x_{jl}$只能取值0或者1。          \n",
    "在伯努利模型中，每个特征的取值是布尔型的，即true和false，或者1和0。             \n",
    "在文本分类中，就是一个特征有没有在一个文档中出现。          \n",
    "                  \n",
    "## 总结:\n",
    "- 一般来说，如果样本特征的分布大部分是连续值，使用GaussianNB会比较好。 \n",
    "- 如果如果样本特征的分布大部分是多元离散值，使用MultinomialNB比较合适。 \n",
    "- 而如果样本特征是二元离散值或者很稀疏的多元离散值，应该使用BernoulliNB。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三.朴素贝叶斯之鸢尾花数据实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3            4\n",
       "0  5.1  3.5  1.4  0.2  Iris-setosa\n",
       "1  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "2  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "3  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "4  5.0  3.6  1.4  0.2  Iris-setosa"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "dataSet =pd.read_csv('/Users/Florian_Gao/Documents/13-菊安酱机器学习/数据集/iris.txt',header = None)\n",
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.切分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\"\"\"\n",
    "函数功能:随机切分训练集和测试集\n",
    "参数说明:\n",
    "    dataSet:输入的数据集\n",
    "    rate:训练集所占比例 \n",
    "返回:切分好的训练集和测试集 \n",
    "\"\"\"\n",
    "def randSplit(dataSet, rate):\n",
    "    l = list(dataSet.index) #提取出索引\n",
    "    random.shuffle(l)#随机打乱索引 \n",
    "    dataSet.index = l#将打乱后的索引重新赋值给原数据集\n",
    "    n = dataSet.shape[0]#总行数\n",
    "    m = int(n * rate)#训练集的数量 \n",
    "    train = dataSet.loc[range(m), :]#提取前m个记录作为训练集\n",
    "    test = dataSet.loc[range(m, n), :] #剩下的作为测试集 \n",
    "    dataSet.index = range(dataSet.shape[0])#更新原数据集的索引 \n",
    "    test.index = range(test.shape[0])#更新测试集的索引\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test= randSplit(dataSet, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.构建高斯朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnb_classify(train,test):\n",
    "    labels = train.iloc[:,-1].value_counts().index #提取训练集的标签种类\n",
    "    #print(labels)\n",
    "    mean =[] #创建mean的list\n",
    "    std =[] #创建方差的list\n",
    "    result = [] #创建新的预测列表\n",
    "    for i in labels:\n",
    "        item = train.loc[train.iloc[:,-1]==i,:] #对于训练集标签进行提取\n",
    "        m = item.iloc[:,:-1].mean() #对每一列的特征进行计算平均值\n",
    "        s = np.sum((item.iloc[:,:-1]-m)**2)/(item.shape[0]) #计算每一类标签种类的每一个特征的平均值\n",
    "        mean.append(m)  #将平均值写入列表\n",
    "        std.append(s) #将方差写入列表\n",
    "    means = pd.DataFrame(mean,index=labels)  #构建df\n",
    "    #print('means \\n',means)\n",
    "    stds = pd.DataFrame(std,index=labels)\n",
    "    for j in range(test.shape[0]):  \n",
    "        iset = test.iloc[j,:-1].tolist()  # to list 转换成list， iset提取test中每个数据的最后分类的特征\n",
    "        #正态分布公式，越靠近平均值方差越大，则说明是这个分类 \n",
    "        #而且这个是和means的每一个每类的每一列的平均值进行计算prob的\n",
    "        iprob = np.exp(-1*(iset-means)**2/(stds*2))/(np.sqrt(2*np.pi*stds)) \n",
    "        #print('iprob=\\n',iprob)\n",
    "        prob = 1 #初始化：概率为1\n",
    "        compare_pro=[-np.exp(1000000),'apple'] #自己添加的一个list，为了比较得出最大的prob\n",
    "        ''''\n",
    "        prob *= iprob[3] #按列计算概率\n",
    "            #选出prob最大的那列的标签 他这个方法就是选最后一列的那个谁大就是谁，跟前三列无关，不知道怎么解释\n",
    "        cla = prob.index[np.argmax(prob.values)] \n",
    "        '''\n",
    "        for k in range(test.shape[1]-1):\n",
    "            prob *= iprob[k] #按列计算概率\n",
    "            #选出prob最大的那列的标签 他这个方法就是选最后一列的那个谁大就是谁，跟前三列无关，不知道怎么解释\n",
    "            cla = prob.index[np.argmax(prob.values)] \n",
    "            if compare_pro[0]<=np.argmax(prob.values): #利用if比较，然后赋值，但是这样子反而出来的概率成功率小于原来的方法\n",
    "                compare_pro[0]=np.argmax(prob.values)\n",
    "                compare_pro[1]=cla\n",
    "            #print('cla=\\n compare_pro=\\n',cla,compare_pro)\n",
    "        result.append(cla)#这样有问题是只记录最后一列的最大的prob的index，将上面进行一个最大值替换,\n",
    "        #result.append(compare_pro[1]) #用这个代码进行做的话，概率不高，但是我觉得应该是正确的\n",
    "        #print('result= \\n',result)\n",
    "    test['predict']=result #添加列\n",
    "    acc = (test.iloc[:,-1]==test.iloc[:,-2]).mean() #看是否相等，求mean\n",
    "    print(f'模型预测准确率为{acc}')\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 测试模型预测效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将切分好的训练集和测试集带入模型，查看模型预测效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型预测准确率为0.9666666666666667\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>6.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3                4          predict\n",
       "0   7.7  3.0  6.1  2.3   Iris-virginica   Iris-virginica\n",
       "1   5.4  3.4  1.7  0.2      Iris-setosa      Iris-setosa\n",
       "2   6.4  2.9  4.3  1.3  Iris-versicolor  Iris-versicolor\n",
       "3   4.6  3.2  1.4  0.2      Iris-setosa      Iris-setosa\n",
       "4   5.1  3.7  1.5  0.4      Iris-setosa      Iris-setosa\n",
       "5   6.7  3.1  4.7  1.5  Iris-versicolor  Iris-versicolor\n",
       "6   6.0  2.2  4.0  1.0  Iris-versicolor  Iris-versicolor\n",
       "7   5.0  2.3  3.3  1.0  Iris-versicolor  Iris-versicolor\n",
       "8   5.5  2.4  3.7  1.0  Iris-versicolor  Iris-versicolor\n",
       "9   5.1  3.8  1.6  0.2      Iris-setosa      Iris-setosa\n",
       "10  4.4  3.0  1.3  0.2      Iris-setosa      Iris-setosa\n",
       "11  5.1  2.5  3.0  1.1  Iris-versicolor  Iris-versicolor\n",
       "12  6.3  2.8  5.1  1.5   Iris-virginica  Iris-versicolor\n",
       "13  5.1  3.4  1.5  0.2      Iris-setosa      Iris-setosa\n",
       "14  7.4  2.8  6.1  1.9   Iris-virginica   Iris-virginica\n",
       "15  6.4  3.2  5.3  2.3   Iris-virginica   Iris-virginica\n",
       "16  5.4  3.4  1.5  0.4      Iris-setosa      Iris-setosa\n",
       "17  4.4  3.2  1.3  0.2      Iris-setosa      Iris-setosa\n",
       "18  5.0  3.4  1.6  0.4      Iris-setosa      Iris-setosa\n",
       "19  6.5  2.8  4.6  1.5  Iris-versicolor  Iris-versicolor\n",
       "20  6.4  2.8  5.6  2.2   Iris-virginica   Iris-virginica\n",
       "21  5.4  3.7  1.5  0.2      Iris-setosa      Iris-setosa\n",
       "22  6.5  3.0  5.5  1.8   Iris-virginica   Iris-virginica\n",
       "23  6.1  3.0  4.9  1.8   Iris-virginica   Iris-virginica\n",
       "24  7.7  2.6  6.9  2.3   Iris-virginica   Iris-virginica\n",
       "25  5.6  2.7  4.2  1.3  Iris-versicolor  Iris-versicolor\n",
       "26  5.7  2.8  4.5  1.3  Iris-versicolor  Iris-versicolor\n",
       "27  5.1  3.3  1.7  0.5      Iris-setosa      Iris-setosa\n",
       "28  5.5  2.3  4.0  1.3  Iris-versicolor  Iris-versicolor\n",
       "29  6.8  3.2  5.9  2.3   Iris-virginica   Iris-virginica"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_classify(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行10次，查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.8666666666666667\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为1.0\n",
      "模型预测准确率为0.9666666666666667\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为1.0\n",
      "模型预测准确率为0.9333333333333333\n",
      "模型预测准确率为0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    " for i in range(10):\n",
    "    train,test= randSplit(dataSet, 0.8)\n",
    "    gnb_classify(train,test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四.使用朴素贝叶斯进行文档分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯一个很重要的应用就是文本分类，所以我们以在线社区留言为例。为了不影响社区的发展，我们要屏蔽 侮辱性的言论，所以要构建一个快速过滤器，如果某条留言使用了负面或者侮辱性的语言，那么就将该留言标志为 内容不当。过滤这类内容是一个很常见的需求。对此问题建立两个类型:侮辱类和非侮辱类，使用1和0分别表示。            \n",
    "我们把文本看成单词向量或者词条向量，也就是说将句子转换为向量。考虑出现所有文档中的单词，再决定将哪些\n",
    "单词纳入词汇表或者说所要的词汇集合，然后必须要将每一篇文档转换为词汇表上的向量。简单起见，我们先假设\n",
    "已经将本文切分完毕，存放到列表中，并对词汇向量进行分类标注。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.构建词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "留言文本已经被切分好，并且人为标注好类别，用于训练模型。类别有两类，侮辱性(1)和非侮辱性(0)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 此案例所有的函数:\n",
    "* loadDataSet:创建实验数据集 \n",
    "* createVocabList:生成词汇表 \n",
    "* setOfWords2Vec:生成词向量 \n",
    "* get_trainMat:所有词条向量列表 \n",
    "* trainNB:朴素贝叶斯分类器训练函数 \n",
    "* classifyNB:朴素贝叶斯分类器分类函数 \n",
    "* testingNB:朴素贝叶斯测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能:创建实验数据集\n",
    "参数说明:无参数\n",
    "返回:\n",
    "postingList:切分好的样本词条\n",
    "classVec:类标签向量 \n",
    "\"\"\"\n",
    "\n",
    "def loadDataSet():\n",
    "    dataSet=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "             ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], \n",
    "             ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "             ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "             ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], \n",
    "             ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']] #切分好的词条\n",
    "    classVec = [0,1,0,1,0,1] #类别标签向量，1代表侮辱性词汇，0代表非侮辱性词汇\n",
    "    return dataSet,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet,classVec=loadDataSet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能:将切分的样本词条整理成词汇表(不重复)\n",
    "参数说明:\n",
    "dataSet:切分好的样本词条 返回:\n",
    "vocabList:不重复的词汇表 \n",
    "\"\"\"\n",
    "\n",
    "def createVocabList(dataSet):\n",
    "    vocabSet = set()  #创建一个空的集合\n",
    "    for doc in dataSet: #for循环遍历其中的每一行\n",
    "        print(doc)\n",
    "        vocabSet = vocabSet | set(doc) #取并集，可以去重\n",
    "        vocabList = list(vocabSet) \n",
    "    return vocabList #得到的vocabList是一个去过重的含有32个元素的list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "['posting', 'stop', 'buying', 'not', 'dalmation', 'quit', 'to', 'steak', 'maybe', 'please', 'park', 'take', 'help', 'worthless', 'stupid', 'mr', 'flea', 'ate', 'my', 'so', 'problems', 'love', 'has', 'is', 'dog', 'him', 'cute', 'garbage', 'how', 'I', 'food', 'licks']\n"
     ]
    }
   ],
   "source": [
    "vocabList = createVocabList(dataSet)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能:根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0 参数说明:\n",
    "vocabList:词汇表\n",
    "inputSet:切分好的词条列表中的一条 返回:\n",
    "returnVec:文档向量,词集模型 \n",
    "\"\"\"\n",
    "\n",
    "def setOfWords2Vec(vocabList, inputSet): #根据vocabList词汇表，将inputSet向量化，向量的每个元素为1或0\n",
    "    returnVec = [0] * len(vocabList) #创建一个其中所含元素都为0的向量 \n",
    "    for word in inputSet:   #遍历每个词条 \n",
    "        if word in vocabList: #如果词条存在于词汇表中，则变为1\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(f\" {word} is not in my Vocabulary!\" )\n",
    "    return returnVec  #返回文档向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有词条向量列表:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能:生成训练集向量列表\n",
    "参数说明:\n",
    "dataSet:切分好的样本词条 返回:\n",
    "trainMat:所有的词条向量组成的列表 \n",
    "\"\"\"\n",
    "\n",
    "def get_trainMat(dataSet):\n",
    "    trainMat = [] #初始化向量列表\n",
    "    vocabList = createVocabList(dataSet) #生成词汇表\n",
    "    for inputSet in dataSet: #遍历样本词条中的每一条样本,也就是dataset的每一行\n",
    "        returnVec=setOfWords2Vec(vocabList, inputSet) #将当前词条向量化\n",
    "        trainMat.append(returnVec) #追加到向量列表中,最后的shape是6✖️32\n",
    "    return trainMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "函数测试运行结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'has', 'flea', 'problems', 'help', 'please']\n",
      "['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid']\n",
      "['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him']\n",
      "['stop', 'posting', 'stupid', 'worthless', 'garbage']\n",
      "['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him']\n",
      "['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1], [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "trainMat=get_trainMat(dataSet)\n",
    "print(trainMat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.朴素贝叶斯分类器训练函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量构建好之后，我们就可以来构建朴素贝叶斯分类器的训练函数了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能:朴素贝叶斯分类器训练函数\n",
    "参数说明:\n",
    "    trainMat:训练文档矩阵\n",
    "    classVec:训练类别标签向量 \n",
    "返回:\n",
    "    p0V:非侮辱类的条件概率数组 \n",
    "    p1V:侮辱类的条件概率数组\n",
    "    pAb:文档属于侮辱类的概率 \n",
    "\"\"\"\n",
    "def trainNB(trainMat,classVec):\n",
    "    n = len(trainMat) #计算训练的文档数目 \n",
    "    print('行数',n)\n",
    "    m = len(trainMat[0])#计算每篇文档的词条数  一共6行，第一行有多少元素\n",
    "    print('列数',m)\n",
    "    pAb = sum(classVec)/n #文档属于侮辱类的概率 \n",
    "    p0Num = np.zeros(m) #词条出现数初始化为0 \n",
    "    p1Num = np.zeros(m) #词条出现数初始化为0 \n",
    "    p0Denom = 0 #分母初始化为0\n",
    "    p1Denom = 0 #分母初始化为0\n",
    "    for i in range(n):  #遍历每一个文档\n",
    "        print('i=',i)\n",
    "        if classVec[i] == 1: #统计属于侮辱类的条件概率所需的数据\n",
    "            p1Num += trainMat[i]\n",
    "            print('p1num=',p1Num)\n",
    "            p1Denom += sum(trainMat[i])\n",
    "            print('p1Denom',p1Denom)\n",
    "        else:  #统计属于非侮辱类的条件概率所需的数据\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1V = p1Num/p1Denom\n",
    "    p0V = p0Num/p0Denom\n",
    "    return p0V,p1V,pAb #返回属于非侮辱类,侮辱类和文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试函数，查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行数 6\n",
      "列数 32\n",
      "i= 0\n",
      "i= 1\n",
      "p1num= [0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      "p1Denom 8\n",
      "i= 2\n",
      "i= 3\n",
      "p1num= [1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 0. 1. 0. 0. 0. 0.]\n",
      "p1Denom 13\n",
      "i= 4\n",
      "i= 5\n",
      "p1num= [1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 2. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 2. 1. 0. 1. 0. 0. 1. 0.]\n",
      "p1Denom 19\n"
     ]
    }
   ],
   "source": [
    "p0V,p1V,pAb = trainNB(trainMat,classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['posting', 'stop', 'buying', 'not', 'dalmation', 'quit', 'to', 'steak', 'maybe', 'please', 'park', 'take', 'help', 'worthless', 'stupid', 'mr', 'flea', 'ate', 'my', 'so', 'problems', 'love', 'has', 'is', 'dog', 'him', 'cute', 'garbage', 'how', 'I', 'food', 'licks']\n"
     ]
    }
   ],
   "source": [
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.04166667, 0.        , 0.        , 0.04166667,\n",
       "       0.        , 0.04166667, 0.04166667, 0.        , 0.04166667,\n",
       "       0.        , 0.        , 0.04166667, 0.        , 0.        ,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.125     , 0.04166667,\n",
       "       0.04166667, 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n",
       "       0.08333333, 0.04166667, 0.        , 0.04166667, 0.04166667,\n",
       "       0.        , 0.04166667])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p0V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05263158, 0.05263158, 0.05263158, 0.05263158, 0.        ,\n",
       "       0.05263158, 0.05263158, 0.        , 0.05263158, 0.        ,\n",
       "       0.05263158, 0.05263158, 0.        , 0.10526316, 0.15789474,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.10526316,\n",
       "       0.05263158, 0.        , 0.05263158, 0.        , 0.        ,\n",
       "       0.05263158, 0.        ])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pAb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.测试朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入包\n",
    "from functools import reduce \n",
    "\"\"\"\n",
    "函数功能:朴素贝叶斯分类器分类函数 \n",
    "参数说明:\n",
    "    vec2Classify:待分类的词条数组 \n",
    "    p0V:非侮辱类的条件概率数组\n",
    "    p1V:侮辱类的条件概率数组\n",
    "    pAb:文档属于侮辱类的概率 返回:\n",
    "    0:属于非侮辱类\n",
    "    1:属于侮辱类 \n",
    "\"\"\"\n",
    "\n",
    "def classifyNB(vec2Classify, p0V, p1V, pAb):\n",
    "    p1 = reduce(lambda x,y:x*y, vec2Classify * p1V) * pAb #对应元素相乘\n",
    "    p0 = reduce(lambda x,y:x*y, vec2Classify * p0V) * (1 - pAb)\n",
    "    print('p0:',p0)\n",
    "    print('p1:',p1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能:朴素贝叶斯测试函数\n",
    "参数说明:\n",
    "testVec:测试样本 返回:测试样本的类别\n",
    "\"\"\"\n",
    "def testingNB(testVec):\n",
    "    dataSet,classVec = loadDataSet() #创建实验样本 \n",
    "    vocabList = createVocabList(dataSet) #创建词汇表\n",
    "    trainMat= get_trainMat(dataSet) #将实验样本向量化 \n",
    "    p0V,p1V,pAb = trainNB(trainMat,classVec) #训练朴素贝叶斯分类器 \n",
    "    thisone = setOfWords2Vec(vocabList, testVec) #测试样本向量化\n",
    "    if classifyNB(thisone,p0V,p1V,pAb): #执行分类并打印分类结果\n",
    "        print(testVec,'属于侮辱类') \n",
    "    else:  #执行分类并打印分类结果\n",
    "        print(testVec,'属于非侮辱类')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "p0: 0.0\n",
      "p1: 0.0\n",
      "['stupid', 'garbage'] 属于非侮辱类\n"
     ]
    }
   ],
   "source": [
    "#测试样本1\n",
    "testVec1 = ['love', 'my', 'dalmation'] \n",
    "testingNB(testVec1)\n",
    "#测试样本2\n",
    "testVec2 = ['stupid', 'garbage'] \n",
    "testingNB(testVec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会发现，这样写的算法无法进行分类，p0和p1的计算结果都是0，显然结果错误。这是为什么呢?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 朴素贝叶斯改进之拉普拉斯平滑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用贝叶斯分类器对文档进行分类时，要计算多个概率的乘积以获得文档属于某个类别的概率，即计算 p(w0|1)p(w1|1)p(w2|1)。如果其中有一个概率值为0，那么最后的成绩也为0。显然，这样是不合理的，为了降低 这种影响，可以将所有词的出现数初始化为1，并将分母初始化为2。这种做法就叫做拉普拉斯平滑(Laplace Smoothing)又被称为加1平滑，是比较常用的平滑方法，它就是为了解决0概率问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外一个遇到的问题就是下溢出，这是由于太多很小的数相乘造成的。我们在计算乘积时，由于大部分因子都很 小，所以程序会下溢或者得不到正确答案。为了解决这个问题，对乘积结果取自然对数。通过求对数可以避免下溢 出或者浮点数舍入导致的错误。同时，采用自然对数进行处理不会有任何损失。下图给出函数f(x)和ln(f(x))的曲 线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查这两条曲线就会发现它们在相同区域内同时增加或者减少，并且在相同点上取到极值。它们的取值虽然不同，\n",
    "但不影响最终结果。因此可以修改代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    " def trainNB(trainMat,classVec):\n",
    "    n = len(trainMat) #计算训练文本的数量\n",
    "    m = len(trainMat[0]) #计算每篇文档的词条数\n",
    "    pAb = sum(classVec)/n #文档属于侮辱类的概率\n",
    "    p0Num = np.ones(m) #词条出现数初始化为1\n",
    "    p1Num = np.ones(m) #词条出现数初始化为1\n",
    "    p0Denom = 2 #分母初始化为2\n",
    "    p1Denom = 2 #分母初始化为2\n",
    "    for i in range(n): #遍历整个文档\n",
    "        if classVec[i] == 1: #统计属于侮辱类的条件概率所需的数据\n",
    "            p1Num += trainMat[i] \n",
    "            p1Denom += sum(trainMat[i])\n",
    "        else: #统计属于非侮辱类的条件概率所需的数据\n",
    "            p0Num += trainMat[i]\n",
    "            p0Denom += sum(trainMat[i])\n",
    "    p1V = np.log(p1Num/p1Denom) #这里做了改变\n",
    "    p0V = np.log(p0Num/p0Denom)\n",
    "    return p0V,p1V,pAb  #返回属于非侮辱类,侮辱类和文档属于侮辱类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看代码结果\n",
    "p0V,p1V,pAb = trainNB(trainMat,classVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0V, p1V, pAb):\n",
    "    p1 = sum(vec2Classify * p1V) + np.log(pAb)\n",
    "    p0 = sum(vec2Classify * p0V) + np.log(1- pAb)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] 属于非侮辱类\n",
      "['stupid', 'garbage'] 属于侮辱类\n"
     ]
    }
   ],
   "source": [
    "#代码运行\n",
    "#测试样本1\n",
    "testVec1 = ['love', 'my', 'dalmation'] \n",
    "testingNB(testVec1)\n",
    "#测试样本2\n",
    "testVec2 = ['stupid', 'garbage'] \n",
    "testingNB(testVec2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
