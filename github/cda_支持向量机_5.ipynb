{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、 什么是SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机(Support Vector Machine,SVM)是用于分类的一种算法，也属于有监督学习的范畴.                \n",
    "<font color='red'>概述一下:</font>              \n",
    "当一个分类问题，数据是线性可分(linearly separable)的，也就是用一根棍就可以将两种小球分开的时候，我们 只要将棍的位置放在让小球距离棍的距离最大化的位置即可，寻找这个最大间隔的过程，就叫做<font color='red'>最优化</font>。但是，现实往往是很残酷的，一般的数据是线性不可分的，也就是找不到一个棍将两种小球很好的分类。这个时候，我们就 需要像大侠一样，将小球拍起，用一张纸代替小棍将小球进行分类。想要让数据飞起，我们需要的东西就是<font color='red'>核函数(kernel)</font>，用于切分小球的纸，就是<font color='red'>超平面(hyperplane)</font>。<font color='green'>如果数据集是N维的，那么超平面就是N-1维的。</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>具有“最大间隔”的超平面就是SVM要寻找的最优解</font>。而这个真正的最优解对应的两侧虚线所穿过的样本点，就是SVM中的<font color='red'>支持样本点</font>，称为“支持向量(support vector)”。支持向量到超平面的距离被称为<font color='red'>间隔(margin)</font>。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、线性SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个最优化问题通常有两个最基本的因素: \n",
    "- 1) 目标函数，也就是你希望什么东西的什么指标达到最好; \n",
    "- 2) 优化对象，你期望通过改变哪些因素来使你的目标函数达到最优。           \n",
    "\n",
    "在线性SVM算法中，目标函数显然就是那个“间隔”，而优化对象则是超平面。我们以线性可分的二分类问题为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.超平面方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在线性可分的二分类问题中，超平面其实就是一条直线。相信直线方程大家都不陌生:\n",
    "$y=ax+b$(公式1)            \n",
    "现在我们做个小小的改变，让原来的 轴变成 轴， 变成 轴，于是公式(1)中的直线方程会变成下面的样子:             \n",
    "$x_2=ax_1+b$(公式2)        \n",
    "$ax_1+(-1)x_2+b=0$(公式3)               \n",
    "向量形式写成：\n",
    "\n",
    "\\begin{align}\n",
    "(a+b)^{\\prime}&=(BA)^{\\prime}=B((AB)^{\\prime})^2A\\nonumber\\\\\n",
    "&=\\left(\n",
    "\\begin{array}{cc}\n",
    "b & 1\\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "&\\left(\n",
    "\\begin{array}{cc}\n",
    "b^{\\prime} & z\\\\\n",
    "0 & a^{\\prime}\\\\\n",
    "\\end{array}\n",
    "\\right)^2\n",
    "&\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "a \\\\\n",
    "\\end{array}\n",
    "\\right)\\nonumber\\\\\n",
    "&=b^{\\prime}+bb^{\\prime}za+bza^{\\prime}a+a^{\\prime}\\nonumber\\\\\n",
    "&=(1-bb^{\\prime})[\\sum\\limits_{i=0}^sb^i(a^{\\prime})^{i+1}]+[\\sum\\limits_{i=0}^r(b^{\\prime})^{i+1}a^i](1-aa^{\\prime})\\nonumber\\\\\n",
    "&=(1-bb^{\\prime})[\\sum\\limits_{i=0}^{s-1}b^i(a^{\\prime})^{i+1}]+[\\sum\\limits_{i=0}^{r-1}(b^{\\prime})^{i+1}a^i](1-aa^{\\prime}) \\nonumber \\qquard \\#\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 什么是SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO算法就是序列最小优化(Sequential Minimal Optimization)，它是由 John Platt 于1996年发布的专门用于 训练SVM的一个强大算法。SMO算法的目的是将大优化问题分解为多个小优化问题来求解。这些小优化问题往往 很容易求解，并且对它们进行顺序求解的结果与将它们作为整体来求解的结果完全一致的。在结果完全相同的同 时，SMO算法的求解时间短很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMO算法的目标是求出一系列$\\alpha$和b，一旦求出了这些，就很容易计算出权重向量$w$并得到分隔超平面。              \n",
    "SMO算法的工作原理是:每次循环中选择两个 进行优化处理。一旦找到了一对合适的 ，那么就增大其中一个同 时减小另一个。这里所谓的\"合适\"就是指两个$\\alpha$必须符合以下两个条件:\n",
    " * 两个$\\alpha$必须要在间隔边界之外\n",
    " * 这两个$\\alpha$还没有进行过区间化处理或者不在边界上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SMO算法流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 简化版SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 SMO算法的伪代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建一个α向量并初始化为0向量 \n",
    "- 当迭代次数 < 最大迭代次数时<font color='red'>(外循环)</font>:\n",
    "    - 对数据集中每个数据向量<font color='red'>(内循环)</font>: \n",
    "        - 如果该数据向量可以被优化:\n",
    "             * 随机选择另外一个数据向量\n",
    "             * 同时优化这两个向量\n",
    "             * 如果两个向量都不能被优化，则退出内循环\n",
    "    - 如果所有向量都没被优化，迭代次数+1，继续下一次循环"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 构建辅助函数         \n",
    "生成特征向量和标签向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\" \n",
    "函数功能:\n",
    "    创建特征向量和标签向量 \n",
    "参数说明:\n",
    "    file:原始文件路径 \n",
    "返回:\n",
    "    xMat:特征向量\n",
    "    yMat:标签向量 \n",
    "\"\"\"\n",
    "def loadDataSet(file):\n",
    "   dataSet= pd.read_table(file,header = None)\n",
    "   xMat=np.mat(dataSet.iloc[:,:-1].values)\n",
    "   yMat=np.mat(dataSet.iloc[:,-1].values).T\n",
    "   return xMat,yMat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def showDataSet(xMat, yMat):\n",
    "    data_p = []  #正样本\n",
    "    data_n = []   #负样本\n",
    "    m = xMat.shape[0] #样本总数\n",
    "    for i in range(m):\n",
    "        if yMat[i] > 0:\n",
    "            data_p.append(xMat[i])\n",
    "        else:\n",
    "            data_n.append(xMat[i])\n",
    "    data_p_ = np.array(data_p)   #转换为numpy矩阵 \n",
    "    data_n_ = np.array(data_n)    #转换为numpy矩阵 \n",
    "    plt.scatter(data_p_.T[0], data_p_.T[1])   #正样本散点图\n",
    "    plt.scatter(data_n_.T[0], data_n_.T[1])   #负样本散点图\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机选择$\\alpha$对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\"\"\" \n",
    "函数功能:\n",
    "    随机选择一个索引 \n",
    "参数说明:\n",
    "    i:第一个alpha索引\n",
    "    m:数据集总行数 \n",
    "返回:\n",
    "    j:随机选择的不与i相等的值 \n",
    "\"\"\"\n",
    "def selectJrand(i,m):\n",
    "    j=i\n",
    "    while (j==i):\n",
    "        j=int(random.uniform(0,m))\n",
    "    return j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha_j$的修剪函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "函数功能:\n",
    "    修剪alpha_j \n",
    "\"\"\"\n",
    "def clipAlpha(aj,H,L):\n",
    "    if aj>H:\n",
    "        aj=H\n",
    "    if L>aj:\n",
    "        aj=L\n",
    "    return aj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 简化版SMO算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数功能: \n",
    "参数说明:\n",
    "    xMat:特征向量 \n",
    "    yMat:标签向量\n",
    "    C:常数\n",
    "    toler:容错率 \n",
    "    maxIter:最大迭代次数\n",
    "返回: \n",
    "    b、alpha\n",
    "\"\"\"\n",
    "def smoSimple(xMat,yMat,C,toler,maxIter):\n",
    "    b=0  #初始化b参数\n",
    "    m,n=xMat.shape    #m为数据集的总行数，n为特征的数量\n",
    "    alpha = np.mat(np.zeros((m,1)))   #初始化alpha参数，设为0 \n",
    "    iters =0   #初始化迭代次数\n",
    "    while (iters<maxIter):\n",
    "        alpha_ = 0   #初始化alpha优化次数\n",
    "        for i in range(m):\n",
    "            #步骤1:计算误差Ei \n",
    "            fXi=np.multiply(alpha,yMat).T*(xMat*xMat[i,:].T)+b \n",
    "            Ei=fXi-yMat[i]\n",
    "            #优化alpha，设定容错率\n",
    "            if ((yMat[i]*Ei<-toler)and(alpha[i]<C)) or((yMat[i]*Ei>toler)and(alpha[i]>0)): \n",
    "                #随机选择一个与alpha_i成对优化的alpha_j\n",
    "                j=selectJrand(i,m)\n",
    "                #步骤1:计算误差Ej \n",
    "                fXj=np.multiply(alpha,yMat).T*(xMat*xMat[j,:].T)+b \n",
    "                Ej=fXj-yMat[j]\n",
    "                #保存更新前的alpha_i和alpha_j \n",
    "                alphaIold=alpha[i].copy() \n",
    "                alphaJold=alpha[j].copy()\n",
    "                #步骤2:计算上下界H和L\n",
    "                if (yMat[i]!=yMat[j]):\n",
    "                    L=max(0,alpha[j]-alpha[i])\n",
    "                    H=min(C,C+alpha[j]-alpha[i])\n",
    "                else:\n",
    "                    L=max(0,alpha[j]+alpha[i]-C)\n",
    "                    H=min(C,C+alpha[j]+alpha[i])\n",
    "                if L==H:\n",
    "                    #print('L==H')\n",
    "                    continue\n",
    "                    #步骤3:计算学习率eta(eta是alpha_j的最优修改量) \n",
    "                    eta=2*xMat[i,:]*xMat[j,:].T-xMat[i,:]*xMat[i,:].T-xMat[j,:]*xMat[j,:].T \n",
    "                    if eta>=0:\n",
    "                        #print('eta>=0')\n",
    "                        continue\n",
    "                    #步骤4:更新alpha_j\n",
    "                    alpha[j]-= yMat[j]*(Ei-Ej)/eta #步骤5:修剪alpha_j\n",
    "                    alpha[j]=clipAlpha(alpha[j],H,L)\n",
    "                    if abs(alpha[j]-alphaJold)<0.00001:\n",
    "                        #print('alpha_j 变化太小')\n",
    "                        continue\n",
    "                    #步骤6:更新alpha_i\n",
    "                    alpha[i]+=yMat[j]*yMat[i]*(alphaJold-alpha[j])\n",
    "                    #步骤7:更新b_1和b_2 \n",
    "                    b1=b-Ei-yMat[i]*(alpha[i]-alphaIold)*xMat[i,:]*xMat[i,:].T-yMat[j]*(alpha[j]-alphaJold)*xMat[i,:]*xMat[j,:].T\n",
    "                    b2=b-Ej-yMat[i]*(alpha[i]-alphaIold)*xMat[i,:]*xMat[j,:].T-yMat[j]*(alpha[j]-alphaJold)*xMat[j,:]*xMat[j,:].T \n",
    "                    #步骤8:根据b_1和b_2更新b\n",
    "                    if (0<alpha[i])and(C>alpha[i]): \n",
    "                        b=b1 \n",
    "                    elif (0<alpha[j])and(C>alpha[j]): \n",
    "                        b=b2 \n",
    "                    else: \n",
    "                        b=(b1+b2)/2\n",
    "                    #统计优化次数\n",
    "                    alpha_+=1\n",
    "                    #print(f'第{iters}次迭代 样本{i},alpha优化次数:{alpha_}') \n",
    "            #更新迭代次数\n",
    "            if alpha_==0: \n",
    "                iters+=1 \n",
    "            else: \n",
    "                iters=0\n",
    "                #print(f'迭代次数为:{iters}')\n",
    "        return b,alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xMat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xMat' is not defined"
     ]
    }
   ],
   "source": [
    "%time b,alpha=smoSimple(xMat,yMat,0.6,0.001,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
